{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "A Decision Tree is a machine learning algorithm that makes decisions by splitting data into smaller groups based on feature values. It works like a flowchart with branches.\n",
        "\n",
        "In classification:\n",
        "\n",
        "- The tree starts at the root node (the whole dataset).\n",
        "\n",
        "- It selects the best feature to split the data (using criteria like Gini impurity or entropy).\n",
        "\n",
        "- The data is divided into branches based on the selected feature.\n",
        "\n",
        "- This process continues until the tree reaches leaf nodes, where a final class label is assigned.\n",
        "\n",
        "In simple terms, a Decision Tree classifies new data by following the path of decisions from the root to a leaf, based on feature values.\n",
        "\n",
        "\n",
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "**Gini Impurity**\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen sample would be incorrectly classified if it were labeled according to the class distribution in that node.\n",
        "\n",
        "* Formula:\n",
        " Gini=1‚àí‚àëpi^2‚Äã\n",
        "  \n",
        "  (where ùëùi^2 is the probability of each class)\n",
        "\n",
        "* **Lower Gini = purer node**\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "Entropy measures the level of uncertainty or disorder in the data.\n",
        "\n",
        "* Formula:\n",
        "  [\n",
        "  Entropy = ‚àí‚àëpi ‚Äãlog2 ‚Äã(pi‚Äã)\n",
        "  ]\n",
        "\n",
        "* **Lower entropy = purer node**\n",
        "\n",
        "\n",
        "**How They Impact Splits in a Decision Tree**\n",
        "\n",
        "* During training, the decision tree algorithm checks all possible splits.\n",
        "* For each split, it calculates **Gini** or **Entropy**.\n",
        "* It chooses the split that produces the **lowest impurity** (i.e., the purest child nodes).\n",
        "* This means the chosen split should separate the classes as well as possible.\n",
        "\n",
        "In simple terms, **both impurity measures help the tree pick the best feature and threshold to create clean, well-separated groups**.\n",
        "\n",
        "\n",
        "**Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "**Pre-Pruning (Early Stopping)**\n",
        "\n",
        "Pre-pruning stops the tree from growing too deep during training.\n",
        "It sets limits such as:\n",
        "\n",
        "* maximum depth\n",
        "* minimum samples required to split\n",
        "* minimum samples per leaf\n",
        "\n",
        "**Advantage:**\n",
        "Faster training, useful when working with large datasets (e.g., customer data with millions of rows).\n",
        "It avoids unnecessary splits and saves computational cost.\n",
        "\n",
        "--\n",
        "\n",
        "**Post-Pruning (Pruning After Training)**\n",
        "\n",
        "Post-pruning allows the tree to grow fully first and then trims unnecessary or weak branches.\n",
        "\n",
        "**Advantage:**\n",
        "Produces a simpler and more accurate model, especially helpful when the initial tree overfits (e.g., medical diagnosis datasets that are noisy).\n",
        "\n",
        "**Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "Information Gain is a metric used in Decision Trees to measure how much a feature improves the purity of the dataset after a split.\n",
        "\n",
        "It is calculated as:\n",
        "\n",
        "**Information Gain = Entropy(before split)‚àíEntropy(after split)**\n",
        "\n",
        "Why It Is Important:\n",
        "\n",
        "- Information Gain helps the tree choose the best feature to split the data.\n",
        "\n",
        "- A split with high Information Gain means:\n",
        "\n",
        "- It reduces uncertainty the most\n",
        "\n",
        "- It creates child nodes that are more pure\n",
        "\n",
        "- It separates the classes better\n",
        "\n",
        "In simple words, Information Gain tells the decision tree which feature gives the most useful separation, helping build a better and more accurate model.\n",
        "\n",
        "\n",
        "**Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "**Real-World Applications of Decision Trees**\n",
        "\n",
        "1. **Loan approval (Banking)**\n",
        "   Used to decide whether a person qualifies for a loan based on income, credit score, etc.\n",
        "\n",
        "2. **Medical diagnosis (Healthcare)**\n",
        "   Helps classify whether a patient has a disease based on symptoms and test results.\n",
        "\n",
        "3. **Fraud detection (Finance & E-commerce)**\n",
        "   Identifies unusual or suspicious transactions.\n",
        "\n",
        "4. **Customer segmentation (Marketing)**\n",
        "   Groups customers based on age, spending habits, or behavior for targeted marketing.\n",
        "\n",
        "5. **Predicting churn (Telecom)**\n",
        "   Determines if a customer is likely to leave the service.\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "1. **Easy to understand and interpret**\n",
        "   Looks like a flowchart, so even non-technical people can understand the model.\n",
        "\n",
        "2. **Works with both numerical and categorical data**\n",
        "   No need for heavy preprocessing.\n",
        "\n",
        "3. **Handles non-linear relationships**\n",
        "   Can capture complex decision boundaries.\n",
        "\n",
        "4. **Requires less data preparation**\n",
        "   No need for scaling or normalization.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "1. **Prone to overfitting**\n",
        "   Trees can become too deep and memorize the data instead of learning patterns.\n",
        "\n",
        "2. **Unstable**\n",
        "   Small changes in data can create a completely different tree.\n",
        "\n",
        "3. **Biased toward features with many categories**\n",
        "   Features with more levels may dominate the splits.\n",
        "\n",
        "4. **Lower accuracy compared to ensemble methods**\n",
        "   Models like Random Forest or XGBoost perform better in most cases.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4wOh2Fv1Fqli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "'''"
      ],
      "metadata": {
        "id": "YErzFK3eLvoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n",
        "\n",
        "'''\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # features\n",
        "y = iris.target     # labels\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITbByATgJ1mt",
        "outputId": "1a134472-04c6-4abc-fe32-ac5808d4d4d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Model 1: Decision Tree with max_depth=3\n",
        "# ------------------------------\n",
        "dt_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_limited.fit(X_train, y_train)\n",
        "pred_limited = dt_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, pred_limited)\n",
        "\n",
        "# ------------------------------\n",
        "# Model 2: Fully-grown Decision Tree (no depth limit)\n",
        "# ------------------------------\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy (max_depth=3):\", accuracy_limited)\n",
        "print(\"Accuracy (Fully-grown tree):\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xtR-zmcL1PN",
        "outputId": "23bbf886-bdf2-4c0f-c309-49df0f5e26d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (Fully-grown tree): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "‚óè Load the Boston Housing Dataset\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n",
        "'''\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "df[\"target\"] = housing.target\n",
        "\n",
        "# Splitting data\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print output\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, reg.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o82QstseMMBl",
        "outputId": "8cca7b13-1fd5-4285-877b-bccbcf0f150a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split (correct 4 outputs)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    \"max_depth\": [1, 2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-28LzPjM1bh",
        "outputId": "06c58c97-f5f0-475b-b2b0-646286f54322"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:**\n",
        "\n",
        "- Handle the missing values\n",
        "\n",
        "- Encode the categorical features\n",
        "\n",
        "- Train a Decision Tree model\n",
        "\n",
        "- Tune its hyperparameters\n",
        "\n",
        "- Evaluate its performance\n",
        "\n",
        "**And describe what business value this model could provide in the real-world\n",
        "setting.**\n",
        "\n",
        "*Answer*\n",
        "\n",
        "**1. Handling missing values:**\n",
        "I would start by identifying the amount and pattern of missing data.\n",
        "\n",
        "* For **numerical features**, I'd use **median imputation**.\n",
        "* For **categorical features**, I'd replace missing values with a **'Missing'** category.\n",
        "* If missingness itself is meaningful, I'd also add **missing-indicator columns**.\n",
        "  All imputations would be done inside a **pipeline** to avoid data leakage.\n",
        "\n",
        "**2. Encoding categorical features:**\n",
        "\n",
        "* For low-cardinality categories ‚Üí **One-Hot Encoding**\n",
        "* For high-cardinality categories ‚Üí **Frequency or Target Encoding**\n",
        "  Encoding is also applied inside the pipeline so it fits **only on training data**.\n",
        "\n",
        "**3. Training a Decision Tree model:**\n",
        "I would build a pipeline with preprocessing + 'DecisionTreeClassifier', split data using **stratified train-test**, apply **class weights** if the dataset is imbalanced, and then fit the model on the processed data.\n",
        "\n",
        "**4. Hyperparameter tuning:**\n",
        "I would use **GridSearchCV** or **RandomizedSearchCV** to tune key parameters like:\n",
        "\n",
        "* 'max_depth'\n",
        "* 'min_samples_split'\n",
        "* 'min_samples_leaf'\n",
        "* 'criterion'\n",
        "  The scoring metric would depend on the problem, e.g., **Recall or F1** because false negatives are costly in healthcare.\n",
        "\n",
        "**5. Evaluating performance:**\n",
        "\n",
        "I would evaluate using:\n",
        "\n",
        "* **Accuracy**, **Precision**, **Recall**, **F1-score**,\n",
        "* **ROC-AUC** (or **PR-AUC** for imbalance),\n",
        "* A **confusion matrix** to understand false negatives and false positives.\n",
        "  If required, I'd calibrate the model's probabilities.\n",
        "\n",
        "**6. Business value in real-world healthcare:**\n",
        "This model can help **early disease detection**, prioritize high-risk patients, reduce unnecessary tests, support doctors in decision-making, and ultimately **improve patient outcomes while reducing healthcare costs**.\n",
        "\n"
      ],
      "metadata": {
        "id": "0aVgo_0nO1YD"
      }
    }
  ]
}